Chaque donnée est un match. Pour chaque donnée (parmi n = 12 303 données) on a accès à :
- l'id du match
- la league du match (non accessible dans le test)
- le noms des 2 équipes (non accessible dans le test)
- pour chacune des deux équipes, on a des statistiques (team features) sur la (précédente?) saison de l'équipe.
    Ces statistiques sont sous la forme de 25 métriques agrégés de 6 manière différente (la moyenne, la somme, la std, la moyenne des 5 derniers matchs, et le std des 5 derniers matchs)
    Parmi ces 25 métriques, les 5 dernières sont aggrégés sous seulement 4 manière différente.
    Cela donne au total 20 * 6 + 5 * 4 = 140 team features, donc 280 bi-team features (auquel on peut ajouter pour chaque équipe le ID du match, la league du match et le nom de l'équipe, les deux premières étant les mêmes pour les deux équipes).
- pour chaque joueur de chaque équipe (environ 19 joueurs par équipe, variable) (via l'ID de la ligne qui correspond au match dans lequel il a joué), on a 
    - la league du match (non accessible dans le test)
    - le nom de l'équipe (non accessible dans le test)
    - la position (GK, DEF, MID, ATT) du joueur
    - le nom du joueur (non accessible dans le test)
    - des statistiques (player features) sur la (précédente?) saison du joueur.
        Ces statistiques sont sous la forme de 51 métriques agrégés de 6 manière différente (la moyenne, la somme, la std, la moyenne des 5 derniers matchs, et le std des 5 derniers matchs)
        Parmi ces 51 métriques, les 2 dernières sont aggrégés sous seulement 4 manière différente.
        Cela donne au total 49 * 6 + 2 * 4 = 302 player features (auquel on peut ajouter pour chaque joueur le ID du match, la league du match, le nom de l'équipe, le nom du joueur).


Questions :
- Est-ce que les données sont ordonnées par date ou autre chose ?
- Pour une même équipe, les team features ne sont pas tout le temps les mêmes pour deux matchs différents, qu'est ce que ca veut dire ?
- Est ce qu'un joueur peut avoir plusieurs roles dans sa carrière (ATT et DEF) ?
- Pourquoi pour une team feature donnée, la season_average n'est pas proportionnelle à la season_sum ?
        Conclusion : 
                    Parce que chaque équipe a un nombre différent de matchs dans la saison, donc la moyenne est pas proportionnelle à la somme.
                    En revanche, pour 5 last match avg vs 5 last match sum, les distributions originales (avant anonymisations) devraient etre proportionelle (par 5), et si c'est une normalisation minmax par exemple, ca devrait conserver la proportionnalité
- Différences entre les distributions de train et de test ?
        Conclusion : 
                    teamfeatures : (r~2.5) donc distributions différentes
                    playerfeatures : (r~0.5) donc distributions identiques et non indépendantes
                        La différence normalisé est meme inférieur à 1 ce qui est pas censé être le cas si les deux distributions empiriques sont issus de données iid, ce qui indique que pour les players, le train et test dataset sont non disjoints !
- Différences entre les distributions de HOME et de AWAY ?
        Conclusion : 
                    teamfeatures : presque tous les ratios sont supérieurs à 1 (r~1.2) mais pas de beaucoup, j'aurais tendance à dire que les deux distributions sont les memes
                    playerfeatures : (r~0.5) donc distributions identiques et non indépendantes : players apparaissant dans les deux distributions
- Differences entre les distributions agrégés de la meme métrique ?
        Conclusion : 
                    teamfeatures : pour de nombreuses métriques, on observe une loss normalisé r assez faible (r~2) ce qui indique des distributions proches alors que ca n'est pas censé l'etre entre le std et l'average
                        Pour certaines agg functions, c'est normal (sum et average) mais pour d'autres (std et average) c'est pas normal                    
                        Une team qui a une feature j average sur la dernière saison élevé (relativement aux autres data team), aura souvent un std élevé également (relativement aux autres data team), ce que je n'explique pas
                        J'aurais tendance à faire attention aux métriques qui voient leurs std corrélés à leur average/sum
                    playerfeatures : meme constant, avec des r encore plus faibles...

Remarques :
- on a des données manquantes parfois
- les distributions des team features ont souvent la meme forme (log normale ?) avec un pic à 3 et étant entre 0 et 10, elles sont normalisées. Les red cards par contre sont pas log normale mais plutot bimodale avec des équipes sages et des équipes dites "un peu fofolles". 
- d'après l'exemple, la plus haute feature importance c'est la std du nombre de yellow cards de la saison... (overfitting ?)

Pistes :
- Possibilité de retrouver le nom de l'équipe d'une équipe du test, à l'aide des données de train (classification) ?
- Meme chose pour les joueurs ?
- Système de elo ?

Feature engineering :
- teamfeatures t_H and t_A. (2 m_team features)
- adversarial teamfeatures t_H - t_A. (m_team features)
- generalized teamfeatures R(t_H, t_H) and R(t_A, t_A) (2 m_team² features), where R(x,y)jj' = x_j - x_j'
- adversarial generalized teamfeatures R(t_H, t_A). (m_team² features)

- role-base-aggregation playerfeatures ap_H and ap_A. (2*n_agg*n_roles*m_player features) = 2*m_aplayers where :
    - n_agg is the number of way we could aggregate features
    - n_roles is the number of roles (GK, ATK,...) including VOID (no role) and ALL (any player)
    We call those features aplayerfeatures, noted ap_H and ap_A. We can operate the same operations as for teamfeatures on those features.
- adversarial aplayerfeatures ap_H - ap_A. (m_aplayers features)
- generalized aplayerfeatures R(ap_H, ap_H) and R(ap_A, ap_A) (2*m_aplayers² features)
- adversarial generalized aplayerfeatures R(ap_H, ap_A). (m_aplayers² features)